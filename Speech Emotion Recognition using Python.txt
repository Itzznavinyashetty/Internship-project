Speech Emotion Recognition using Python | Sound Classification | Deep Learning Project Tutorial
Updated: Jun 1

Unleash the power of speech emotion recognition with Python! This comprehensive tutorial explores sound classification and deep learning techniques for decoding emotions from speech. Learn to build accurate models that can detect and classify emotions in spoken words, opening doors to applications in psychology, customer service, and more. Enhance your skills in audio processing, machine learning, and dive into the fascinating world of deep learning. Decode the emotions hidden in speech with this hands-on project tutorial. #SpeechEmotionRecognition #Python #SoundClassification #DeepLearning #AudioProcessing #MachineLearning

In this project tutorial we are going to analyze and classify various audio files to a corresponding class and visualize the frequency of the sounds through a plot.


Dataset Information

There are a set of 200 target words were spoken in the carrier phrase "Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 data points (audio files) in total.


The dataset is organized such that each of the two female actor and their emotions are contain within its own folder. And within that, all 200 target words audio file can be found. The format of the audio file is a WAV format


Output Attributes
anger

disgust

fear

happiness

pleasant surprise

sadness

neutral


Import Modules

pandas - used to perform data manipulation and analysis

numpy - used to perform a wide variety of mathematical operations on arrays

matplotlib - used for data visualization and graphical plotting

os - used to handle files using system commands

seaborn - built on top of matplotlib with similar functionalities

librosa - used to analyze sound files

librosa.display - used to display sound data as images

Audio - used to display and hear the audio

warnings - to manipulate warnings details


Load the Dataset

Dataset is Loaded

The paths of the speech data has been loaded for further processing

Filenames were split and appended as labels

To ensure proper processing all characters were converted to lower case


len(paths)
2800

No. of samples in the dataset

paths[:5]

First five path files in the dataset

labels[:5]
First five labels of the speech files in the dataset


Now we create a dataframe of the audio files and labels

## Create a dataframe
df = pd.DataFrame()
df['speech'] = paths
df['label'] = labels
df.head()

File path is the input data

Label is the output data


df['label'].value_counts()
fear       400
angry      400
disgust    400
neutral    400
sad        400
ps         400
happy      400
Name: label, dtype: int64

List of classes in the data set and the amount of samples per class


Exploratory Data Analysis

All classes in equal distribution

For unequal distribution, you must balance the distribution between classes


Now we define the functions for the waveplot and spectrogram

Waveplot is to view the waveform of the audio file

Spectrogram is to view the frequency levels of the audio file


Waveplot and spectrogram of an audio file from each class is plotted

Sample audio of emotion speech from each class is displayed

Lower pitched voices have darker colors

Higher pitched voices have more brighter colors


Feature Extraction

Now we define a feature extraction function for the audio files

Audio duration capped to max 3 seconds for equal duration of file size

It will extract the Mel-frequency cepstral coefficients (MFCC) features with the limit of 40 and take the mean as the final feature

extract_mfcc(df['speech'][0])
Feature values of an audio file
Returns extracted features from all the audio files
Visualization of the features extracted from the data

The more samples in the dataset, the longer the processing time

X = [x for x in X_mfcc]
X = np.array(X)
X.shape
(2800, 40)

Conversion of the list into a single dimensional array
## input split
X = np.expand_dims(X, -1)
X.shape
(2800, 40, 1)

The shape represents the number of samples in the dataset and features in a single dimension array


from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder()
y = enc.fit_transform(df[['label']])
y = y.toarray()
y.shape
(2800, 7)

The shape represents the number of samples and number of output classes

Create the LSTM Model

Dense - single dimension linear layer with hidden units

Dropout - used to add regularization to the data, avoiding over fitting & dropping out a fraction of the data

Loss='sparse_categorical_crossentropy' - computes the cross-entropy loss between true labels and predicted labels.

Optimizer='adam' - automatically adjust the learning rate for the model over the number of epochs


Now we train the model

Display of the results during each epoch of training

batch_size=64 - amount of data to process per step

epochs=50 - no. of iterations for training the model

validation_split=0.2 - train and test split percentage

The training accuracy and validation accuracy increases each iteration

best validation accuracy is 72.32

use checkpoint to save the best validation accuracy model

adjust learning rate for slow convergence



Final Thoughts
Deep learning models give more accuracy results compared to machine learning algorithms

Sound features are extracted and used for training the speech emotion recognition model

More training data will get you better accuracy

This model can be reused differently depending on the data set and parameters, including speech recognition or other sound related tracks


In this project tutorial, we have explored the Speech Emotion Recognition dataset as a classification project under deep learning. Different speech emotion sounds were identified and classified with explanatory data analysis


